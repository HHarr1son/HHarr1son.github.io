---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hello everyone, this is Harrison's personal academic homepage. My research interests include neural decoding and EEG image and video reconstruction.


# üî• News
- *2024.12*: &nbsp;üéâüéâ Icassp2025 
- *2025.02*: &nbsp;üéâüéâ Information Fusion
- *2025.04*: &nbsp;üéâüéâ IEEE Transactions on Fuzzy Systems
- *2025.05*: &nbsp;üéâüéâ Engineering Applications of Artificial Intelligence

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Information Fusion</div><img src='images/1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CCSUMSP: A cross-subject Chinese speech decoding framework with unified topology and multi-modal semantic pre-training]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))

**Shuai Huang**, Yongxiong Wang, Huan Luo

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EAAI</div><img src='images/hh1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Dual-Branch Generative Adversarial Network with Self-Supervised Enhancement for Robust Auditory Attention Decoding]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TFS</div><img src='images/qc1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Active Domain Adaptation Based on Probabilistic Fuzzy c-means Clustering for Pancreatic Tumor Segmentation]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP2025</div><img src='images/hh4.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SSAAD: A Multi-Scale Temporal-Frequency Graph Network for Binary Auditory Attention Detection with Self-Supervised Learning]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arvix</div><img src='images/hh3.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MINDEV: Multi-modal Integrated Diffusion Framework for Video Reconstruction from EEG Signals]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arvix</div><img src='images/hh2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MIRVIS: EEG-based Multi-Integration Reconstruction Architecture for Visual Imagery Synthesis]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arvix</div><img src='images/hh6.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MindIntent:Zero-shotVisualIntentCross-domainAlignmentand RepresentationinHumanBrain]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arvix</div><img src='images/hh7.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FocusWhereItMatters:LLM-GuidedAttentionPriorsforFew-Shot Segmentation]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arvix</div><img src='images/hh8.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MindVAR:Time-sensitive Cross-modal Video Autoregressive Reconstruction from Human Brain Activity]([https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000958))
</div>
</div>



- [SVTNet: Dual Branch of Swin Transformer and Vision Transformer for Monocular Depth Estimation] **Icassp 2025**
 

# üéñ Honors and Awards


# üìñ Educations
- *2023.06 - now*, Department of Control Science and Engineering, University of Shanghai for Science and Technology.
  
